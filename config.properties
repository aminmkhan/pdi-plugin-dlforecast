# No need to have every property set
# You can leave out any of the configurations and they will be set to their default values

# An epoch is a full pass of the dataset. Too few epochs won't give your network enough time to learn good parameters.
# Too many epochs might result in overfitting.
nEpochs=50

# timeSeriesSize is the size of each timeseries example, meaning the number of timesteps that are used as data to
# predict the timestep ahead. Increasing it too much will make training slower.
timeSeriesSize=30

# A minibatch refers to the number of examples used at a time, when computing gradients and parameter updates.
# Try using power of 2 values in the range of [16-128] to enable a faster training
# If you have a small dataset you can try using a value smaller than 16 has having a
# too big or too small miniBatchSize might lead to overfitting.
miniBatchSize=32

# Learning rate affects how well the network will learn from the data
# Typical values for the learning rate are in the range of 0.1 to 1.0E-6
learningRate=0.001

# Number of nodes in the hidden layer, more nodes will make training longer
# Too few nodes might increase error
# Too many might lead to overfitting and poor generalization
hiddenNodes=5

# Available updaters NESTEROVS (default momentum of 0.9), RMSPROP (default decay of 0.95 and epsilon of 1.0E-8)
# and ADAGRAD (default epsilon of 1.0E-6)
updater=NESTEROVS
# If you use NESTEROVS or RMSPROP you can configure their hyper parameters.
momentum=0.9
rmsDecay=0.95

# Available activation functions suited for LSTM's - TANH or SOFTSIGN
activation=TANH

# Regularization helps the model generalize and prevents overfitting
# Set regularization to true if you wish to use l1, l2
regularization=false
# In comparison to l2, l1 isn't really used but the option is still given to the user
l1=0.0
# Common values for l2 regularization are 1.0E-3 to 1.0E-6
l2=0.0

# From dl4j documentation
# Dropout probability. This is the probability of retaining an activation.
# So dropOut(x) will keep an activation with probability x, and set to 0 with probability 1-x.
# dropOut(0.0) is disabled (default).
dropOut=0.0

# Truncated back propagation through time user for training networks with long time-series to improve training speed
# updates parameters depending on defined length avoiding a full backward pass through every time step
# each time back propagation is done
truncatedBPTT=false
# Length should be bigger than each time series (fed to the network, value of Timeseries Length) length
# and shorter than the full time-series data set
tBPTTLength=30

# If you're saving your model and want to do further training on the model using another deeplearning4j application
# set this to true
saveUpdater=false